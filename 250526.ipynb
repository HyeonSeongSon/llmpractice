{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182c5d5b",
   "metadata": {},
   "source": [
    "설정 / 앱 / vscode, Miniconda3, python을 제거\n",
    "사용자계정 폴더 .conda .ipython .vscode .miniconda3 폴더외, condarc 파일 제거\n",
    "C:\\User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266f1213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-openai) (1.78.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1996ce5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-p'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.getenv('OPENAI_API_KEY')[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcacbf",
   "metadata": {},
   "source": [
    "Langchain의 핵심 컴포넌트 : 모델 호출계를 구성하는 추상화 요소를 제공\n",
    "```\n",
    "    - PromptTemplate : LLM에 보낼 입력 프롬프트\n",
    "    - ChatOpenAI : openai의 GPT - 모델 호출\n",
    "    - Runnable : 실행가능한 객체에 대한 공통 인터페이스 -> invoke() 매서드를 통해서 입력과 출력을 지원\n",
    "    - StrOutPutParser : 문자열 출력 파서\n",
    "파이프로 연결 가능... ex) prompt | llm | strparser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d3f5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140c724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'커피제품을 생산하는 회사이름은 뭘로 하면 좋을까?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = '{product}제품을 생산하는 회사이름은 뭘로 하면 좋을까?'\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formated_prompt = prompt.format(product = '커피')\n",
    "formated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38804ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BbGg2MWS8ekVWC4EO16lMHli4UAuU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ab0df0e-8b72-46de-aa72-67ed04891e26-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# 위에서 선언했기 때문에 key값을 입력하지 않아도 됨\n",
    "llm=ChatOpenAI(model='gpt-4o-mini', temperature=0) # Runnable 객체 --> invoke()\n",
    "response = llm.invoke([('human','안녕')])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd26410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 출력 파서 Runnable 객체 -invoke\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "parser_text= parser.invoke(response)\n",
    "parser_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732e304",
   "metadata": {},
   "source": [
    "LangChain Expression Language(LCEL) 단일 체인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16974841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 제품을 생산하는 회사 이름으로는 다음과 같은 아이디어를 고려해볼 수 있습니다:\n",
      "\n",
      "1. **커피의 정원** (Garden of Coffee)\n",
      "2. **아침의 향기** (Morning Aroma)\n",
      "3. **커피의 예술** (Art of Coffee)\n",
      "4. **커피 마법** (Coffee Magic)\n",
      "5. **한 잔의 행복** (Cup of Joy)\n",
      "6. **커피 이야기** (Coffee Tales)\n",
      "7. **커피의 여정** (Journey of Coffee)\n",
      "8. **프리미엄 로스트** (Premium Roast)\n",
      "9. **커피의 미소** (Coffee Smile)\n",
      "10. **커피의 순간** (Moments of Coffee)\n",
      "\n",
      "이름은 브랜드의 이미지와 목표에 맞게 선택하는 것이 중요하니, 여러 가지 아이디어를 조합해보거나 변형해보는 것도 좋습니다!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | parser  # runnable 객체 --> invoke\n",
    "result = chain.invoke([\"product\",\"coffee\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422f49f",
   "metadata": {},
   "source": [
    "Langchain을 활용한 모델 사용, 비용 모니터링 및 개성 전략\n",
    "```\n",
    "Langchain을 활용한 모델 사용, 비용모니터링 및 캐싱 전략\n",
    "    GPT-4o-mini GPT 3.5-Turbo 비용이 60% 저렴\n",
    "    Langchain v0.3부터 openAI가 별도 패키지로 분리 필요 패키지를 설치 langchain-openai 필요\n",
    "    토큰 사용량 추정, 캐싱을 위한 LangChain-community도 별도 설치\n",
    "    환경변수 변수 관리 패키지 python-dotevn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb680953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (0.3.18)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-cummunity (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-cummunity\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai langchain-cummunity python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68392bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d74d962a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 다양한 언어 모델과 데이터 소스를 연결하여 자연어 처리 애플리케이션을 쉽게 구축할 수 있도록 지원하는 프레임워크입니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
    "prompt = 'LangChain에 대해 한문장으로 설명해줘'\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5615b8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 18,\n",
       " 'output_tokens': 35,\n",
       " 'total_tokens': 53,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용량\n",
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f768899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1 죄송하지만, 실시간 ...\n",
      "응답2 랭체인(RLangC ...\n",
      "총 토큰수 :  737\n",
      "프롬프트 토큰수 :  37\n",
      "응답토큰수 :  700\n",
      "비용계산(USD) :  0.00042555\n"
     ]
    }
   ],
   "source": [
    "# 콜백함수를 통한 누적 토큰 추적(get_openai_callback)\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    # 첫번째 호출\n",
    "    res1 = llm.invoke('서울의 오늘 날씨는 어떤지 알려줘')\n",
    "    print('응답1', res1.content[:10], '...')\n",
    "    # 두번째 호출\n",
    "    res2 = llm.invoke('파이썬으로 랭체인 사용법 알려줘')\n",
    "    print('응답2', res2.content[:10], '...')\n",
    "\n",
    "# 누적 토큰 사용량 출력 콜백 cb에는 블록 전체 토큰 사용량이 누적\n",
    "# 총 토큰수\n",
    "print('총 토큰수 : ',cb.total_tokens)\n",
    "# 프롬프트 토큰수\n",
    "print('프롬프트 토큰수 : ', cb.prompt_tokens)\n",
    "# 응답 토큰수\n",
    "print('응답토큰수 : ', cb.completion_tokens)\n",
    "# 비용계산\n",
    "print('비용계산(USD) : ', cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494b1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 의 LLM 응답캐싱 (InMemorry Cache, SQLiteCache)\n",
    "# 동일한 질문은 저장해 뒀다가 응답에 사용\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "# InMemorryCache 설정\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b589328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1 : 물고기가 학교에 가면 뭐라고 부를까요?\n",
      "\n",
      "\"피시 스쿨!\" 🐟😄\n",
      "--------------------------------------------------\n",
      "응답2 : 물고기가 학교에 가면 뭐라고 부를까요?\n",
      "\n",
      "\"피시 스쿨!\" 🐟😄\n",
      "--------------------------------------------------\n",
      "응답3 : 물론이죠! \n",
      "\n",
      "왜 컴퓨터는 바다를 싫어할까요?\n",
      "\n",
      "바다에 가면 항상 \"버그\"가 생기니까요! 🐛💻\n",
      "\n",
      "재미있으셨나요? 다른 유머도 원하시면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "# 캐시 사용 전후를 비교, 같은 질문을 두번 호출\n",
    "query = '재미있는 유머 하나 알려줘'\n",
    "# 첫 번째 호출 (캐시에 없으면 api 호출 발생)\n",
    "result1 = llm.invoke(query)\n",
    "print(f'응답1 : {result1.content}')\n",
    "print('-'*50)\n",
    "# 두 번째 호출(동일한 query, 캐서를 확인하고 동일 질문하면 api 미호출)\n",
    "result2 = llm.invoke(query)\n",
    "print(f'응답2 : {result2.content}')\n",
    "print('-'*50)\n",
    "query = query + '?'\n",
    "# 세 번째 호출(!가 추가된 query)\n",
    "result3 = llm.invoke(query)\n",
    "print(f'응답3 : {result3.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9b8aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 호출시간 : 4.591396808624268\n",
      "두번째 호출시간 : 0.0009007453918457031\n"
     ]
    }
   ],
   "source": [
    "# 실행시간 측정\n",
    "import time\n",
    "# 첫 번째 호출 시간\n",
    "query = '오늘 점심 메뉴 추천 해줘!'\n",
    "start = time.time(); llm.invoke(query); end = time.time()\n",
    "print(f'첫번째 호출시간 : {end-start}')\n",
    "\n",
    "start = time.time(); llm.invoke(query); end = time.time()\n",
    "print(f'두번째 호출시간 : {end-start}')\n",
    "\n",
    "# InMemorry 방식은 어플리케이션을 종료하면 저장된 내용이 날라간다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de89fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 호출시간 : 4.121056795120239\n",
      "응답1 : 물론이죠! 저녁 메뉴로 몇 가지 추천해드릴게요.\n",
      "\n",
      "1. **된장찌개와 밥**: 한국의 전통적인 된장찌개에 밥을 곁들여 간단하면서도 맛있는 한 끼를 즐길 수 있습니다.\n",
      "\n",
      "2. **불고기**: 양념된 소고기를 볶아내어 쌈채소와 함께 먹으면 훌륭한 저녁이 됩니다.\n",
      "\n",
      "3. **잡채**: 당면과 다양한 채소, 고기를 볶아 만든 잡채는 맛도 좋고 영양도 풍부합니다.\n",
      "\n",
      "4. **해물파전**: 비 오는 날에 특히 잘 어울리는 해물파전은 간단하게 만들 수 있고, 막걸리와 함께하면 더욱 좋습니다.\n",
      "\n",
      "5. **치킨**: 바삭하게 튀긴 치킨에 다양한 사이드 메뉴를 곁들여 즐길 수 있는 저녁도 좋습니다.\n",
      "\n",
      "어떤 메뉴가 마음에 드세요? 추가로 다른 아이디어가 필요하면 말씀해 주세요!\n",
      "두 번째 호출시간 : 0.0030121803283691406\n",
      "응답2 : 물론이죠! 저녁 메뉴로 몇 가지 추천해드릴게요.\n",
      "\n",
      "1. **된장찌개와 밥**: 한국의 전통적인 된장찌개에 밥을 곁들여 간단하면서도 맛있는 한 끼를 즐길 수 있습니다.\n",
      "\n",
      "2. **불고기**: 양념된 소고기를 볶아내어 쌈채소와 함께 먹으면 훌륭한 저녁이 됩니다.\n",
      "\n",
      "3. **잡채**: 당면과 다양한 채소, 고기를 볶아 만든 잡채는 맛도 좋고 영양도 풍부합니다.\n",
      "\n",
      "4. **해물파전**: 비 오는 날에 특히 잘 어울리는 해물파전은 간단하게 만들 수 있고, 막걸리와 함께하면 더욱 좋습니다.\n",
      "\n",
      "5. **치킨**: 바삭하게 튀긴 치킨에 다양한 사이드 메뉴를 곁들여 즐길 수 있는 저녁도 좋습니다.\n",
      "\n",
      "어떤 메뉴가 마음에 드세요? 추가로 다른 아이디어가 필요하면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "# 임시로 In-Memory 캐시로 설정하여 DB 연결을 끊기\n",
    "# set_llm_cache(InMemoryCache())\n",
    "\n",
    "\n",
    "# SQLite 캐시(디스크기반 캐시)\n",
    "import os, time\n",
    "from langchain_community.cache import SQLiteCache\n",
    "# 기존 캐시 DB 삭제(.langchain.db 초기화)\n",
    "\n",
    "# if os.path.exists('.langchain.db'):\n",
    "#     os.remove('.langchain.db')\n",
    "\n",
    "# SQLite 캐시 설정(지정한 경로의 DB 파일을 생성 / 사용)\n",
    "set_llm_cache(SQLiteCache(database_path = '.langchain.db')) # langchain.db 있으면 사용 없으면 생성\n",
    "\n",
    "# 동일한 query를 두면 호출해서 결과와 시간을 비교\n",
    "query = '저녁메뉴 추천해줘'\n",
    "# 첫 번째 호출(캐시에 없으면 api 호출 발생)\n",
    "start = time.time(); result1 = llm.invoke(query); end = time.time()\n",
    "print(f'첫 번째 호출시간 : {end-start}')\n",
    "print(f'응답1 : {result1.content}')\n",
    "\n",
    "# 두 번째 호출(캐시에 없으면 api 호출 발생)\n",
    "start = time.time(); result2 = llm.invoke(query); end = time.time()\n",
    "print(f'두 번째 호출시간 : {end-start}')\n",
    "print(f'응답2 : {result2.content}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d2394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa3bdb95",
   "metadata": {},
   "source": [
    "LangChain 프롬프트 템플릿\n",
    "```\n",
    "LLM 프롬프트를 동적으로 구성하고 재 사용할 수 있도록 해주는 도구\n",
    "    단일입력 : 하나의 변수로 구성된 프롬프트 템플릿\n",
    "    다중입력 : 둘 이상의 변수를 사용하는 템플릿\n",
    "    ChatPromptTemplate 역할 기반 프롬프트 : 시스템/사용처 역할별 프롬프트 구성 .from_massage()\n",
    "    PartialPromptTemplate 활용 : 프롬프트 일부를 미리 고정하고 부분포멧팅을 사용해(ex 시스템 메세지는 고정...)\n",
    "    프롬프트 출력 및 체인 실행 : LCEL\n",
    "    프롬프트 작성 팁: 주의사항 및 모범사례\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f6588c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement python-devenv (from versions: none)\n",
      "ERROR: No matching distribution found for python-devenv\n"
     ]
    }
   ],
   "source": [
    "# 환경설정\n",
    "%pip install langchain langchain-openai python-devenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c61c4046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84db3e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 최고 수준의 마케팅 카피라이터 입니다.\n",
      "아래 제품의 매렺거인 홍부문굴르 100자 내외로 작성해 주세요\n",
      "\n",
      "제품명 : 이클립스\n"
     ]
    }
   ],
   "source": [
    "# 단일 프롬프트 사용\n",
    "from langchain.prompts import PromptTemplate\n",
    "# 템플릿 문자열 정의\n",
    "template_str = (\n",
    "    \"당신은 최고 수준의 마케팅 카피라이터 입니다.\\n\"\n",
    "    \"아래 제품의 매렺거인 홍부문굴르 100자 내외로 작성해 주세요\\n\\n\"\n",
    "    \"제품명 : {product_name}\"\n",
    ")\n",
    "# promptTemplate 객체 생성\n",
    "product_prompt = PromptTemplate.from_template(template_str)\n",
    "# 템플릿에 값 채우기\n",
    "formated_prompt = product_prompt.format(product_name = '이클립스')\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "447e2abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"하루의 시작을 특별하게! 깊고 풍부한 향의 커피 한 잔으로 당신의 감각을 깨워보세요. 매일의 일상에 작은 행복을 더하는 커피, 지금 바로 경험해보세요!\"\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 | llm --> invoke\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "# Runnable 객체 생성  LCEL\n",
    "chain = product_prompt | llm\n",
    "result = chain.invoke({'product_name':'커피'})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86541f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 뉴스기사 제목과 키워드 입니다.\n",
      "이 정보를 바탕으로 한 문단으로 구성된 간락한 요약문을 작성하세요\n",
      "\n",
      "제목:인공지능 기술의 발전과 미래\n",
      "키워드:머신러닝,딥러닝,LLM,랭체인,산업 혁신\n"
     ]
    }
   ],
   "source": [
    "# 다중입력\n",
    "# 다중입력 템플릿 문자열 저의\n",
    "multi_tmeplate_str = (\n",
    "    \"아래는 뉴스기사 제목과 키워드 입니다.\\n\"\n",
    "    \"이 정보를 바탕으로 한 문단으로 구성된 간락한 요약문을 작성하세요\\n\\n\"\n",
    "    \"제목:{title}\\n\"\n",
    "    \"키워드:{keyword}\"\n",
    ")\n",
    "# 프롬프트 템플릿 작성\n",
    "summary_prompt = PromptTemplate(template=multi_tmeplate_str, input_variables=['title','keyword'])\n",
    "# 포멧팅을 통해 프롬프트 값 확인\n",
    "sample_title = '인공지능 기술의 발전과 미래'\n",
    "sample_keyword = '머신러닝,딥러닝,LLM,랭체인,산업 혁신'\n",
    "formatted_summary_prompt = summary_prompt.format(title=sample_title, keyword =sample_keyword)\n",
    "print(formatted_summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3808da19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능 기술의 발전은 머신러닝과 딥러닝의 혁신을 통해 가속화되고 있으며, 특히 대규모 언어 모델(LLM)과 랭체인 기술이 주목받고 있습니다. 이러한 기술들은 다양한 산업 분야에서 혁신을 이끌어내고 있으며, 기업들은 이를 활용하여 효율성을 높이고 새로운 비즈니스 모델을 창출하고 있습니다. 앞으로 인공지능의 지속적인 발전은 우리의 삶과 산업 구조에 큰 변화를 가져올 것으로 기대됩니다.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL 출력\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "result_summary = (summary_prompt | llm | parser).invoke({\n",
    "    'title': sample_title,\n",
    "    'keyword' : sample_keyword\n",
    "})\n",
    "result_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b29dfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate과 역할 기반 프롬프트\n",
    "# 시스템/사용자/어시스턴트 역할(role)\n",
    "# 시스템 메시지 : 모델의 동작을 지시\n",
    "# 사용자 메세지 : 실제 사용자의 입력\n",
    "# 어시스턴트 메시지 : 이전 모델이 응답한 내용이 있으면 대화맥락유지를 위해 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45c45ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 python 분야의 뛰어난 전문가이자 조언자 입니다.사용자의 프로그래밍 질문에 대해 친절하고 이해하기 쉽게 단변해 주세요', additional_kwargs={}, response_metadata={}), HumanMessage(content='파이썬에 클래스에 대해 설명해주세요', additional_kwargs={}, response_metadata={})]\n",
      "파이썬에서 클래스는 객체 지향 프로그래밍(OOP)의 기본 개념 중 하나로, 객체를 생성하기 위한 청사진(템플릿)입니다. 클래스를 사용하면 관련된 데이터(속성)와 기능(메서드)을 하나의 단위로 묶을 수 있습니다.\n",
      "\n",
      "### 클래스의 기본 구조\n",
      "\n",
      "클래스를 정의하려면 `class` 키워드를 사용합니다. 다음은 간단한 클래스의 예입니다:\n",
      "\n",
      "```python\n",
      "class Dog:\n",
      "    # 생성자: 객체가 생성될 때 호출되는 메서드\n",
      "    def __init__(self, name, age):\n",
      "        self.name = name  # 속성: 개의 이름\n",
      "        self.age = age    # 속성: 개의 나이\n",
      "\n",
      "    # 메서드: 개가 짖는 행동을 정의\n",
      "    def bark(self):\n",
      "        return f\"{self.name} says woof!\"\n",
      "\n",
      "# 클래스 사용\n",
      "my_dog = Dog(\"Buddy\", 3)  # Dog 클래스의 인스턴스 생성\n",
      "print(my_dog.bark())       # \"Buddy says woof!\" 출력\n",
      "```\n",
      "\n",
      "### 주요 개념\n",
      "\n",
      "1. **속성 (Attributes)**: 클래스의 데이터. 위 예제에서 `name`과 `age`가 속성입니다.\n",
      "2. **메서드 (Methods)**: 클래스의 기능. `bark` 메서드는 개가 짖는 행동을 정의합니다.\n",
      "3. **생성자 (Constructor)**: `__init__` 메서드는 객체가 생성될 때 호출되어 초기 속성을 설정합니다.\n",
      "4. **인스턴스 (Instance)**: 클래스를 기반으로 생성된 객체입니다. 위 예제에서 `my_dog`가 인스턴스입니다.\n",
      "\n",
      "### 클래스의 장점\n",
      "\n",
      "- **재사용성**: 클래스를 정의하면 여러 개의 객체를 쉽게 생성할 수 있습니다.\n",
      "- **조직화**: 관련된 데이터와 기능을 하나의 단위로 묶어 코드의 가독성을 높입니다.\n",
      "- **상속**: 기존 클래스를 기반으로 새로운 클래스를 만들 수 있어 코드의 중복을 줄이고 확장성을 높입니다.\n",
      "\n",
      "클래스는 파이썬에서 매우 중요한 개념이며, 객체 지향 프로그래밍의 핵심입니다. 더 궁금한 점이 있으면 언제든지 질문해 주세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "system_message = (\"당신은 python 분야의 뛰어난 전문가이자 조언자 입니다.\"\n",
    "                  \"사용자의 프로그래밍 질문에 대해 친절하고 이해하기 쉽게 단변해 주세요\")\n",
    "user_message = \"{question}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',system_message),\n",
    "    ('user', user_message)\n",
    "])\n",
    "\n",
    "sample_question = '파이썬에 클래스에 대해 설명해주세요'\n",
    "formated_massage = chat_prompt.format_messages(question=sample_question)\n",
    "print(formated_massage)\n",
    "# 파이프라인을 이용해서 llm호출 및 파싱\n",
    "answer = (chat_prompt | llm | parser).invoke({'question': sample_question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0ec4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParitalPromptTemplate : 템플릿의 일부를 부분적으로 채운 새로운 템플릿\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "role_system_template = '당신은 {role} 분야의 전문 지식인 입니다. 가능한 자세히 답변해 주세요'\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(role_system_template)\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "\n",
    "# chatprompttemplate을 생성\n",
    "base_chat_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "\n",
    "partial_chat_prompt= base_chat_prompt.partial(role = '주식투자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c341dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 주식투자 분야의 전문 지식인 입니다. 가능한 자세히 답변해 주세요', additional_kwargs={}, response_metadata={}), HumanMessage(content='현재 2025년 5월 시장 상황에서 삼성전자 주식 전망은?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# partial로 생성된 프롬프트에 질문만 채워 프롬프트 구성\n",
    "sample_question = '현재 2025년 5월 시장 상황에서 삼성전자 주식 전망은?'\n",
    "message = partial_chat_prompt.format_messages(question = sample_question)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5753d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025년 5월의 삼성전자 주식 전망을 평가하기 위해서는 여러 가지 요소를 고려해야 합니다. 다음은 삼성전자의 주식 전망에 영향을 미칠 수 있는 주요 요인들입니다.\n",
      "\n",
      "1. **반도체 시장 동향**: 삼성전자는 세계 최대의 반도체 제조업체 중 하나입니다. 반도체 수요는 AI, 클라우드 컴퓨팅, IoT(사물인터넷) 등 다양한 산업의 성장에 따라 증가하고 있습니다. 2025년에는 이러한 기술들이 더욱 발전할 것으로 예상되므로, 반도체 수요가 지속적으로 증가할 가능성이 높습니다. 그러나 반도체 시장은 경기 사이클에 민감하므로, 글로벌 경제 상황에 따라 변동성이 클 수 있습니다.\n",
      "\n",
      "2. **스마트폰 및 전자제품 판매**: 삼성전자는 스마트폰, 가전제품 등 다양한 전자제품을 제조하고 있습니다. 5G와 같은 새로운 기술의 도입이 소비자 수요에 긍정적인 영향을 미칠 수 있습니다. 또한, 새로운 제품 라인업이나 혁신적인 기술이 출시될 경우, 시장 점유율을 높일 수 있는 기회가 될 것입니다.\n",
      "\n",
      "3. **글로벌 경제 상황**: 글로벌 경제의 성장률, 금리, 인플레이션 등은 삼성전자의 실적에 큰 영향을 미칠 수 있습니다. 특히, 미국과 중국 간의 무역 관계, 유럽의 경제 상황 등은 삼성전자의 수출에 영향을 미칠 수 있습니다.\n",
      "\n",
      "4. **경쟁 상황**: 삼성전자는 애플, 화웨이, TSMC 등과 같은 강력한 경쟁자들과 경쟁하고 있습니다. 경쟁사의 기술 발전이나 가격 전략에 따라 삼성전자의 시장 점유율과 수익성이 영향을 받을 수 있습니다.\n",
      "\n",
      "5. **ESG(환경, 사회, 지배구조) 경영**: 최근 투자자들은 ESG 요소를 중요하게 고려하고 있습니다. 삼성전자가 지속 가능한 경영을 통해 긍정적인 이미지를 구축하고, 투자자들의 신뢰를 얻는다면 주가에 긍정적인 영향을 미칠 수 있습니다.\n",
      "\n",
      "6. **주가 평가**: 삼성전자의 주가는 PER(주가수익비율), PBR(주가순자산비율) 등 다양한 지표를 통해 평가할 수 있습니다. 현재 주가가 과대평가 또는 과소평가되어 있는지를 분석하는 것도 중요합니다.\n",
      "\n",
      "결론적으로, 삼성전자의 주식 전망은 긍정적일 수 있지만, 글로벌 경제 상황, 경쟁 환경, 기술 발전 등 다양한 외부 요인에 따라 변동성이 클 수 있습니다. 따라서, 투자 결정을 내리기 전에 충분한 분석과 정보를 바탕으로 신중하게 접근하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "answer = (partial_chat_prompt | llm | parser).invoke({'question': sample_question})\n",
    "print(answer) # 2025년 5월을 말하고 있지만 해당모델은 과거의 데이터로 학습한 모델로 신뢰할 수 없는 답변임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fa656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4976c3ce",
   "metadata": {},
   "source": [
    "LangchainMemory \n",
    "```\n",
    "이전 대화 내용을 기억해서 문맥을 유지하는 역활 LangChain 0, 3X부터는 LCEL 기반으로 체인을 구성\n",
    "3X부터는 LCEL 기반으로 체인을 구성 , RunnableWithMessageHistory, ChatMessageHistory 등의 컴포넌트를 활용해서 세션별 대화기록을 관리, 대화가 장기화될 경우 요약 메모리를 도입해서 과거 대화를 LLM으로 요약하고 축약된 형태로 저장해서 프롬프트의 길이 문제를 해결\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d515880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_redis\n",
      "  Downloading langchain_redis-0.2.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting anyio<5.0.0,>=4.9.0 (from langchain_redis)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi<2026.0.0,>=2025.4.26 (from langchain_redis)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore<2.0.0,>=1.0.9 (from langchain_redis)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.6 (from langchain_redis)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain_redis) (0.3.61)\n",
      "Collecting numpy<3,>=2 (from langchain_redis)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting python-ulid<4,>=3 (from langchain_redis)\n",
      "  Downloading python_ulid-3.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting redisvl<0.7.0,>=0.6.0 (from langchain_redis)\n",
      "  Downloading redisvl-0.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain_redis) (9.1.2)\n",
      "Collecting typing-extensions<5.0.0,>=4.13.2 (from langchain_redis)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting urllib3<3.0.0,>=2.4.0 (from langchain_redis)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from anyio<5.0.0,>=4.9.0->langchain_redis) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from anyio<5.0.0,>=4.9.0->langchain_redis) (1.3.0)\n",
      "Collecting h11>=0.16 (from httpcore<2.0.0,>=1.0.9->langchain_redis)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from jinja2<4.0.0,>=3.1.6->langchain_redis) (3.0.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain_redis) (0.3.42)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain_redis) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain_redis) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain_redis) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain_redis) (2.11.3)\n",
      "Collecting jsonpath-ng<2.0.0,>=1.5.0 (from redisvl<0.7.0,>=0.6.0->langchain_redis)\n",
      "  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from redisvl<0.7.0,>=0.6.0->langchain_redis) (0.5.1)\n",
      "Collecting redis<6.0,>=5.0 (from redisvl<0.7.0,>=0.6.0->langchain_redis)\n",
      "  Downloading redis-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_redis) (3.0.0)\n",
      "Collecting ply (from jsonpath-ng<2.0.0,>=1.5.0->redisvl<0.7.0,>=0.6.0->langchain_redis)\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.3->langchain_redis) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.3->langchain_redis) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.3->langchain_redis) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.3->langchain_redis) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.3->langchain_redis) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3->langchain_redis) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3->langchain_redis) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3->langchain_redis) (0.4.0)\n",
      "Collecting PyJWT~=2.9.0 (from redis<6.0,>=5.0->redisvl<0.7.0,>=0.6.0->langchain_redis)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\env1\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.3->langchain_redis) (3.3.2)\n",
      "Downloading langchain_redis-0.2.1-py3-none-any.whl (31 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 20.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.6 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 14.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading python_ulid-3.0.0-py3-none-any.whl (11 kB)\n",
      "Downloading redisvl-0.6.0-py3-none-any.whl (151 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n",
      "Downloading redis-5.3.0-py3-none-any.whl (272 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Installing collected packages: ply, urllib3, typing-extensions, python-ulid, PyJWT, numpy, jsonpath-ng, jinja2, h11, certifi, redis, httpcore, anyio, redisvl, langchain_redis\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.5\n",
      "    Uninstalling Jinja2-3.1.5:\n",
      "      Successfully uninstalled Jinja2-3.1.5\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.1.31\n",
      "    Uninstalling certifi-2025.1.31:\n",
      "      Successfully uninstalled certifi-2025.1.31\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.2\n",
      "    Uninstalling anyio-4.6.2:\n",
      "      Successfully uninstalled anyio-4.6.2\n",
      "Successfully installed PyJWT-2.9.0 anyio-4.9.0 certifi-2025.4.26 h11-0.16.0 httpcore-1.0.9 jinja2-3.1.6 jsonpath-ng-1.7.0 langchain_redis-0.2.1 numpy-2.2.6 ply-3.11 python-ulid-3.0.0 redis-5.3.0 redisvl-0.6.0 typing-extensions-4.13.2 urllib3-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Playdata\\miniconda3\\envs\\env1\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Playdata\\miniconda3\\envs\\env1\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-common 1.2 requires numpy<2.1.4,>=1.25.0, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-core 1.2 requires numpy<2.1.4,>=1.25.0, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-features 1.2 requires numpy<2.1.4,>=1.25.0, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires accelerate<1.0,>=0.34.0, but you have accelerate 1.7.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires numpy<2.1.4,>=1.25.0, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires torch<2.6,>=2.2, but you have torch 2.7.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires torchvision<0.21.0,>=0.16.0, but you have torchvision 0.22.0 which is incompatible.\n",
      "autogluon-tabular 1.2 requires numpy<2.1.4,>=1.25.0, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires accelerate<1.0,>=0.34.0, but you have accelerate 1.7.0 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires numpy<2.1.4,>=1.25.0, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires torch<2.6,>=2.2, but you have torch 2.7.0 which is incompatible.\n",
      "category-encoders 2.8.1 requires scikit-learn>=1.6.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "gluonts 0.16.1 requires numpy<2.2,>=1.16, but you have numpy 2.2.6 which is incompatible.\n",
      "mljar-supervised 1.1.17 requires numpy<2,>=1.19.5, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79b482a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "169be051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human : 안녕하세요 제 이름은 홍길동 입니다\n",
      "ai : 안녕하세요 홍길동님! 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "# 메모리 객체 생성\n",
    "history = InMemoryChatMessageHistory()\n",
    "history.add_user_message('안녕하세요 제 이름은 홍길동 입니다')\n",
    "history.add_ai_message('안녕하세요 홍길동님! 무엇을 도와드릴까요?')\n",
    "# 현재까지의 대화 내용 확인\n",
    "for msg in history.messages:\n",
    "    print(f'{msg.type} : {msg.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ca785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redis 기반 채팅 기록 저장소 (나중에 클라우드랑 같이)\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "import os\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "session_id = 'user_123'\n",
    "history = RedisChatMessageHistory(session_id=session_id, redis_url=REDIS_URL)\n",
    "history.add_user_message('안녕하세요 제 이름은 홍길동 입니다')\n",
    "history.add_ai_message('안녕하세요 홍길동님! 무엇을 도와드릴까요?')\n",
    "# 현재까지의 대화 내용 확인\n",
    "for msg in history.messages:\n",
    "    print(f'{msg.type} : {msg.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "03d9be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션기반 다중사용자 메모리 구조 구현 - 다중사용자 챗봇\n",
    "# 핵심 : session_id를 키로 하는 메모리 저장소를 만들고 사용자의 대화는 키별로 저정한다.\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '당신은 뛰어난 한구어 상담 챗봇입니다. 질문에 친절하고 자세히 답변해주세요'),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    ('human','{input}')\n",
    "])\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e031af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7bad4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션별 메모리 저장소를 딕셔너리롤 만들고, 존재하지 않는 새로운 세션 id가 들어오면 InMemorryChatMessageHistory를 생성\n",
    "# get_session_history를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e979be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# 세션 id -> 대화 기록 객체 매핑\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    \"\"\"\n",
    "    세션 ID에 해당하는 대화 기록 객체를 반환 (없으면 새로 생성)\n",
    "    Input:\n",
    "        session\n",
    "    Output:\n",
    "        InMemoryChatMessageHistory\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 메모리를 통합한 체인 래퍼 생성\n",
    "chatbot = RunnableWithMessageHistory(    \n",
    "    chain,\n",
    "    get_session_history,    \n",
    "    input_messages_key = 'input',\n",
    "    history_messages_key='history'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b869df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:46:20 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[user_a] 질문: 안녕하세요, 저는 홍길동입니다. 당신은 누구신가요?\n",
      "[user_a] 챗봇: 안녕하세요, 홍길동님! 저는 여러분의 질문에 답변하고 도움을 드리기 위해 만들어진 AI 상담 챗봇입니다. 궁금한 점이나 도움이 필요한 부분이 있다면 언제든지 말씀해 주세요!\n",
      "\n",
      "16:46:22 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[user_b] 질문: 안녕하세요, 저는 이순신입니다. 당신은 어떤 일을 하시나요?\n",
      "[user_b] 챗봇: 안녕하세요, 이순신님! 저는 여러분의 질문에 답변하고, 다양한 주제에 대해 정보를 제공하는 상담 챗봇입니다. 역사, 과학, 문화, 일상생활 등 여러 분야에 대해 이야기할 수 있습니다. 궁금한 점이나 도움이 필요한 부분이 있다면 언제든지 말씀해 주세요!\n",
      "\n",
      "16:46:26 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[user_a] 질문: 저는 프로그래밍을 배우고 있습니다. 당신은 어떤 일을 하시나요?\n",
      "[user_a] 챗봇: 프로그래밍을 배우고 계시다니 멋지네요! 저는 주로 여러분의 질문에 답변하고, 정보나 조언을 제공하는 역할을 합니다. 프로그래밍 관련 질문이나 학습에 도움이 필요하시면 언제든지 도와드릴 수 있습니다. 어떤 언어를 배우고 계신가요? 또는 어떤 부분에서 도움이 필요하신가요?\n",
      "\n",
      "16:46:28 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[user_b] 질문: 저는 역사에 관심이 많습니다. 당신은 어떤 분야에 관심이 있나요?\n",
      "[user_b] 챗봇: 역사에 관심이 많으시군요! 역사적인 사건이나 인물에 대해 이야기하는 것은 정말 흥미로운 일입니다. 저는 다양한 분야에 대한 정보를 제공할 수 있지만, 특히 사람들과의 소통, 지식 전달, 그리고 문제 해결에 대한 관심이 많습니다. 역사와 관련된 질문이나 특정 사건에 대해 더 알고 싶으신 부분이 있다면 말씀해 주세요! 함께 이야기해 보아요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 두개의 세션을 번갈아가면서 대화  RunnableWithMessageHistory 가 각 세션에 맞는 대화 기록을 관리합니다.\n",
    "sessions = ['user_a','user_b']\n",
    "questions = [\n",
    "    '안녕하세요, 저는 홍길동입니다. 당신은 누구신가요?',   # usre_a 첫번재 질문\n",
    "    '안녕하세요, 저는 이순신입니다. 당신은 어떤 일을 하시나요?', # user_b 첫번째 질문\n",
    "    '저는 프로그래밍을 배우고 있습니다. 당신은 어떤 일을 하시나요?', # user_a 두번째 질문\n",
    "    '저는 역사에 관심이 많습니다. 당신은 어떤 분야에 관심이 있나요?' # user_b 두번째 질문\n",
    "]\n",
    "for i, question in enumerate(questions):\n",
    "    session_id = sessions[i % 2]  # 세션 ID를 번갈아가며 사용\n",
    "    result = chatbot.invoke({'input': question}, config={'configurable': {'session_id': session_id}})\n",
    "    print(f'[{session_id}] 질문: {question}')\n",
    "    print(f'[{session_id}] 챗봇: {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1d0add30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:46:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[user_c] 질문: 저는 철수에요, 반갑습니다.\n",
      "[user_c] 챗봇: 안녕하세요, 철수님! 반갑습니다. 어떻게 도와드릴까요? 궁금한 점이나 이야기하고 싶은 주제가 있다면 말씀해 주세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke({'input': \"저는 철수에요, 반갑습니다.\"}, config={'configurable': {'session_id': 'user_c'}})\n",
    "print(f'[user_c] 질문: 저는 철수에요, 반갑습니다.')\n",
    "print(f'[user_c] 챗봇: {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c322ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:46:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'홍길동님이라고 말씀하셨습니다! 혹시 더 알고 싶으신 내용이나 다른 질문이 있으신가요? 언제든지 말씀해 주세요!'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chatbot.invoke({'input': \"저는 누구라구요?\"}, config={'configurable': {'session_id': 'user_a'}})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "db16b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:47:00 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이순신님이라고 말씀하셨습니다! 이순신은 조선시대의 유명한 장군으로, 임진왜란 동안의 뛰어난 군사 전략과 용맹으로 잘 알려져 있습니다. 혹시 이순신 장군에 대해 더 이야기하고 싶으신가요, 아니면 다른 역사적 인물이나 사건에 대해 궁금한 점이 있으신가요?'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chatbot.invoke({'input': \"저는 누구라구요?\"}, config={'configurable': {'session_id': 'user_b'}})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e0e21fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:47:05 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'철수님이라고 말씀하셨습니다! 혹시 더 궁금한 점이나 다른 이야기를 나누고 싶으신가요? 언제든지 말씀해 주세요!'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chatbot.invoke({'input': \"저는 누구라구요?\"}, config={'configurable': {'session_id': 'user_c'}})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93024a6b",
   "metadata": {},
   "source": [
    "요약 메모리 구현(대화내용 자동 요약)\n",
    "```\n",
    "긴 대화 내용을 모두 프롬프트에 기록하는 것은 비 효율적 -> 프롬프트의 길이 제한에 걸릴 가능성이 있음\n",
    "Conversation Summary Memory\n",
    "0.3x 버전에서는 직접 요약용 제안을 만들어서 ChatMessageHistory에 적용\n",
    "```\n",
    "어떻게 요약?\n",
    "```\n",
    "- 일정길이 이상으로 대화가 누적되면, 과거 대화를 요약해서 핵심내용만 담김\n",
    "- 요약결과를 메모리에 시스템 메세지 등으로 저장 -> 메모리 절약\n",
    "- 새로운 사용자 입력시 요약된 맥락 + 최근 및 메세지만 참고해서 llm 전달\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d3d3a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약용 프롬프트 템플릿\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','당신은 대화 요약 전문가입니다. 대화의 주요 내용을 간결하게 요약해 주세요'),\n",
    "    ('human','{conversation}') # 전체 대화내용을 하나의 문자열로 전달\n",
    "])\n",
    "# LCEL\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cf9e4b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:25:51 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:25:53 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:25:55 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:25:58 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:25:59 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "요약전 user_d의 메모리 메세지 개수 : 30\n"
     ]
    }
   ],
   "source": [
    "# user_d 세션에 대화 내용을 기록 긴 대화 생성\n",
    "long_queries = [\n",
    "    '안녕, 오늘 뭐하려고 했지',\n",
    "    '아 맞다 내일 회의자료 준비해야지, 회의는 몇시지?',\n",
    "    '그 회의에 누가 참석하는지 기억나?',\n",
    "    '단위프로젝트 진행 상황도 공유해야 할까?',\n",
    "    '최근에 이야기 했던 새로운 기능에 대한 업데이는 있어?'\n",
    "]\n",
    "session_id = 'user_d'\n",
    "for q in long_queries:\n",
    "    answer =  chatbot.invoke({'input': q}, config={'configurable': {'session_id': session_id}})\n",
    "    # print(answer)\n",
    "\n",
    "print(f'요약전 user_d의 메모리 메세지 개수 : {len(store[session_id].messages)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7bc0992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 요약 내용 ==\n",
      "대화 요약:\n",
      "\n",
      "- 인간은 오늘의 계획을 묻고, AI는 다양한 활동을 제안함.\n",
      "- 인간은 내일 회의 자료 준비를 언급하며 회의 시간을 물음. AI는 이메일이나 캘린더에서 확인하라고 조언.\n",
      "- 인간은 회의 참석자를 기억하지 못하고, AI는 초대장이나 팀원에게 문의하라고 안내.\n",
      "- 인간은 단위 프로젝트 진행 상황 공유 여부를 질문하고, AI는 공유의 중요성을 강조하며 도움을 제안.\n",
      "- 인간은 새로운 기능에 대한 업데이트를 묻고, AI는 관련 팀원에게 문의하라고 조언하며 회의에서 공유할 것을 추천.\n"
     ]
    }
   ],
   "source": [
    "# 전체 대화 내용을 요약하고 마지막 사용처 질문-답변 쌍만 원본 유지\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "# 요약 대상 대화내용 추출(마지막 QA 쌍 제외한 이전 내용)\n",
    "message = store[session_id].messages\n",
    "if len(message) > 2:\n",
    "   original_dialog = '\\n'.join([f'{msg.type.upper()} : {msg.content}' for msg in message[:-2]])\n",
    "else:\n",
    "   original_dialog = '\\n'.join([f'{msg.type.upper()} : {msg.content}' for msg in message])\n",
    "\n",
    "# llm으로 요약 샘플\n",
    "summary_text = summary_chain.invoke({'conversation':original_dialog})\n",
    "print('== 요약 내용 ==')\n",
    "print(summary_text)\n",
    "# 기존 메모리를 요약으로교체 -> 이전 내용 요약본 +  최근 QA 유지\n",
    "new_history = InMemoryChatMessageHistory()\n",
    "new_history.messages.append(SystemMessage(content=f'요약:{summary_text}'))\n",
    "# 최근 대화의 마지막 QA쌍을 복원\n",
    "if len(message) >= 2:\n",
    "    last_user_msg = message[-2]\n",
    "    last_ai_msg = message[-1]\n",
    "\n",
    "    if isinstance(last_user_msg, HumanMessage):\n",
    "        new_history.add_user_message(last_user_msg.content)\n",
    "    else:\n",
    "        new_history.messages.append(last_user_msg)\n",
    "\n",
    "    if isinstance(last_ai_msg, AIMessage):\n",
    "        new_history.add_ai_message(last_ai_msg)\n",
    "    else:\n",
    "        new_history.messages.append(last_ai_msg)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "46f2d626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(store[session_id].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17bb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
